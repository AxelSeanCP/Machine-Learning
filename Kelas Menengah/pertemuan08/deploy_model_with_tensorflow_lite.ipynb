{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lumPMiYXltai"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.applications import MobileNetV2\n",
        "from tensorflow.keras.layers import Input\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pathlib\n",
        "import zipfile\n",
        "import io\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "04yy1WysCw7H",
        "outputId": "ad054428-de4a-47ff-bec7-0165ed421c00"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "file_path = '/content/drive/MyDrive/dataset/dataset_1.zip'\n",
        "zip_read = zipfile.ZipFile(file_path, 'r')\n",
        "zip_read.extractall('/content/dataset')\n",
        "zip_read.close()"
      ],
      "metadata": {
        "id": "CCb__kCMDOPx"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.listdir('/content/dataset/dataset_1')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v8987qyYbDB2",
        "outputId": "cbff32df-6cfc-4737-bea7-39060f81d96f"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['test', 'train']"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "TRAINING_DIR = '/content/dataset/dataset_1/train'\n",
        "VALIDATION_DIR = '/content/dataset/dataset_1/test'\n",
        "\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1.0/255,\n",
        "    rotation_range=30,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    vertical_flip=True,\n",
        "    fill_mode='nearest'\n",
        ")\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(TRAINING_DIR, class_mode='categorical',target_size=(150,150))\n",
        "\n",
        "validation_datagen = ImageDataGenerator(\n",
        "    rescale=1.0/255\n",
        ")\n",
        "\n",
        "validation_generator = validation_datagen.flow_from_directory(VALIDATION_DIR, class_mode='categorical',target_size=(150,150))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6WdeHhRXeMio",
        "outputId": "05413117-1568-4c28-de5b-76ecb91c922b"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 550 images belonging to 5 classes.\n",
            "Found 122 images belonging to 5 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pre_trained_model = MobileNetV2(weights=\"imagenet\", include_top=False,\n",
        "                                input_tensor=Input(shape=(150,150,3)))\n",
        "\n",
        "for layer in pre_trained_model.layers:\n",
        "  layer.trainable = False\n",
        "\n",
        "last_output = pre_trained_model.output\n",
        "\n",
        "x = tf.keras.layers.Flatten(name=\"Flatten\")(last_output)\n",
        "x = tf.keras.layers.Dropout(0.5)(x)\n",
        "x = tf.keras.layers.Dense(128, activation=\"relu\")(x)\n",
        "x = tf.keras.layers.Dense(5, activation=\"softmax\")(x)\n",
        "\n",
        "model = tf.keras.models.Model(pre_trained_model.input, x)\n",
        "\n",
        "int_lr = 1e-4\n",
        "num_epochs = 30\n",
        "\n",
        "optimizer = tf.optimizers.Adam(learning_rate=int_lr)\n",
        "model.compile(\n",
        "    optimizer=optimizer,\n",
        "    loss=\"categorical_crossentropy\",\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "H = model.fit(train_generator,\n",
        "              epochs=num_epochs,\n",
        "              validation_data=validation_generator)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KqL12_uphX4x",
        "outputId": "a7514036-eb5e-42aa-db9e-a4e9c85a3b43"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:`input_shape` is undefined or non-square, or `rows` is not in [96, 128, 160, 192, 224]. Weights for input shape (224, 224) will be loaded as the default.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/mobilenet_v2/mobilenet_v2_weights_tf_dim_ordering_tf_kernels_1.0_224_no_top.h5\n",
            "9406464/9406464 [==============================] - 0s 0us/step\n",
            "Epoch 1/30\n",
            "18/18 [==============================] - 29s 1s/step - loss: 1.3921 - accuracy: 0.5964 - val_loss: 0.6705 - val_accuracy: 0.8115\n",
            "Epoch 2/30\n",
            "18/18 [==============================] - 24s 1s/step - loss: 0.4982 - accuracy: 0.8382 - val_loss: 0.5328 - val_accuracy: 0.8525\n",
            "Epoch 3/30\n",
            "18/18 [==============================] - 24s 1s/step - loss: 0.3833 - accuracy: 0.8745 - val_loss: 0.4763 - val_accuracy: 0.8689\n",
            "Epoch 4/30\n",
            "18/18 [==============================] - 23s 1s/step - loss: 0.3450 - accuracy: 0.8855 - val_loss: 0.4081 - val_accuracy: 0.8852\n",
            "Epoch 5/30\n",
            "18/18 [==============================] - 22s 1s/step - loss: 0.2452 - accuracy: 0.9073 - val_loss: 0.4904 - val_accuracy: 0.8689\n",
            "Epoch 6/30\n",
            "18/18 [==============================] - 23s 1s/step - loss: 0.2381 - accuracy: 0.9182 - val_loss: 0.5132 - val_accuracy: 0.8689\n",
            "Epoch 7/30\n",
            "18/18 [==============================] - 22s 1s/step - loss: 0.2514 - accuracy: 0.9127 - val_loss: 0.5039 - val_accuracy: 0.8770\n",
            "Epoch 8/30\n",
            "18/18 [==============================] - 21s 1s/step - loss: 0.2326 - accuracy: 0.9218 - val_loss: 0.4564 - val_accuracy: 0.8934\n",
            "Epoch 9/30\n",
            "18/18 [==============================] - 21s 1s/step - loss: 0.1454 - accuracy: 0.9545 - val_loss: 0.4541 - val_accuracy: 0.8934\n",
            "Epoch 10/30\n",
            "18/18 [==============================] - 21s 1s/step - loss: 0.1976 - accuracy: 0.9345 - val_loss: 0.4931 - val_accuracy: 0.8852\n",
            "Epoch 11/30\n",
            "18/18 [==============================] - 20s 1s/step - loss: 0.1802 - accuracy: 0.9345 - val_loss: 0.5091 - val_accuracy: 0.8852\n",
            "Epoch 12/30\n",
            "18/18 [==============================] - 23s 1s/step - loss: 0.1817 - accuracy: 0.9491 - val_loss: 0.6556 - val_accuracy: 0.8770\n",
            "Epoch 13/30\n",
            "18/18 [==============================] - 23s 1s/step - loss: 0.1419 - accuracy: 0.9418 - val_loss: 0.6454 - val_accuracy: 0.8607\n",
            "Epoch 14/30\n",
            "18/18 [==============================] - 23s 1s/step - loss: 0.1523 - accuracy: 0.9545 - val_loss: 0.6760 - val_accuracy: 0.8607\n",
            "Epoch 15/30\n",
            "18/18 [==============================] - 20s 1s/step - loss: 0.1609 - accuracy: 0.9491 - val_loss: 0.6043 - val_accuracy: 0.8852\n",
            "Epoch 16/30\n",
            "18/18 [==============================] - 20s 1s/step - loss: 0.1707 - accuracy: 0.9400 - val_loss: 0.5664 - val_accuracy: 0.8852\n",
            "Epoch 17/30\n",
            "18/18 [==============================] - 23s 1s/step - loss: 0.1123 - accuracy: 0.9636 - val_loss: 0.6208 - val_accuracy: 0.8852\n",
            "Epoch 18/30\n",
            "18/18 [==============================] - 23s 1s/step - loss: 0.1185 - accuracy: 0.9564 - val_loss: 0.5677 - val_accuracy: 0.8934\n",
            "Epoch 19/30\n",
            "18/18 [==============================] - 23s 1s/step - loss: 0.1364 - accuracy: 0.9582 - val_loss: 0.5872 - val_accuracy: 0.8934\n",
            "Epoch 20/30\n",
            "18/18 [==============================] - 23s 1s/step - loss: 0.1117 - accuracy: 0.9582 - val_loss: 0.6553 - val_accuracy: 0.8770\n",
            "Epoch 21/30\n",
            "18/18 [==============================] - 23s 1s/step - loss: 0.0696 - accuracy: 0.9800 - val_loss: 0.5440 - val_accuracy: 0.8770\n",
            "Epoch 22/30\n",
            "18/18 [==============================] - 20s 1s/step - loss: 0.1180 - accuracy: 0.9545 - val_loss: 0.5781 - val_accuracy: 0.8852\n",
            "Epoch 23/30\n",
            "18/18 [==============================] - 23s 1s/step - loss: 0.1512 - accuracy: 0.9455 - val_loss: 0.5264 - val_accuracy: 0.8934\n",
            "Epoch 24/30\n",
            "18/18 [==============================] - 23s 1s/step - loss: 0.0645 - accuracy: 0.9782 - val_loss: 0.5892 - val_accuracy: 0.8770\n",
            "Epoch 25/30\n",
            "18/18 [==============================] - 23s 1s/step - loss: 0.1141 - accuracy: 0.9600 - val_loss: 0.5524 - val_accuracy: 0.8934\n",
            "Epoch 26/30\n",
            "18/18 [==============================] - 23s 1s/step - loss: 0.0809 - accuracy: 0.9691 - val_loss: 0.5672 - val_accuracy: 0.9016\n",
            "Epoch 27/30\n",
            "18/18 [==============================] - 37s 2s/step - loss: 0.0963 - accuracy: 0.9691 - val_loss: 0.5447 - val_accuracy: 0.8934\n",
            "Epoch 28/30\n",
            "18/18 [==============================] - 35s 2s/step - loss: 0.1014 - accuracy: 0.9673 - val_loss: 0.5515 - val_accuracy: 0.8689\n",
            "Epoch 29/30\n",
            "18/18 [==============================] - 31s 2s/step - loss: 0.0581 - accuracy: 0.9764 - val_loss: 0.5399 - val_accuracy: 0.8934\n",
            "Epoch 30/30\n",
            "18/18 [==============================] - 20s 1s/step - loss: 0.1050 - accuracy: 0.9673 - val_loss: 0.5518 - val_accuracy: 0.8934\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# menyimpan model dalam format saved model\n",
        "export_dir = 'saved_model/'\n",
        "tf.saved_model.save(model, export_dir)\n",
        "\n",
        "# convert SavedModel menjadi vegs.tflite\n",
        "converter = tf.lite.TFLiteConverter.from_saved_model(export_dir)\n",
        "tflite_model = converter.convert()\n",
        "\n",
        "tflite_model_file = pathlib.Path('vegs.tflite')\n",
        "tflite_model_file.write_bytes(tflite_model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8IE725dTpTXZ",
        "outputId": "319a4970-2fd3-4f4c-ea39-399af4a31b2a"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "25245276"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    }
  ]
}