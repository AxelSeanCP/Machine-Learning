{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "6lQqjh1YUZYX",
        "j_WJVJ6lUVlD"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Tokenization"
      ],
      "metadata": {
        "id": "6lQqjh1YUZYX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "oodhNtm5OHqe"
      },
      "outputs": [],
      "source": [
        "# tokenization\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = Tokenizer(num_words=15, oov_token=\"-\")"
      ],
      "metadata": {
        "id": "cQsTYEcZOibi"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# text to tokenized\n",
        "teks = [\"Saya Suka Programming\",\n",
        "        \"Programming sangat menyenangkan\",\n",
        "        \"Machine Learning berbeda dengan pemrograman konvensional\",\n",
        "        \"Axel jatuh cinta sama Meltryllis\"]"
      ],
      "metadata": {
        "id": "O8xT9AdDOpp2"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# implement tokenization\n",
        "tokenizer.fit_on_texts(teks)"
      ],
      "metadata": {
        "id": "V-1En9znPHUc"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# change text into sequence\n",
        "sequences = tokenizer.texts_to_sequences(teks)"
      ],
      "metadata": {
        "id": "syV3vfUkPM5Q"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print tokenized word\n",
        "print(tokenizer.word_index)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SPkh3szzPUgs",
        "outputId": "bcc4d57f-2242-446a-9d68-0438c3ceb8e3"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'-': 1, 'programming': 2, 'saya': 3, 'suka': 4, 'sangat': 5, 'menyenangkan': 6, 'machine': 7, 'learning': 8, 'berbeda': 9, 'dengan': 10, 'pemrograman': 11, 'konvensional': 12, 'axel': 13, 'jatuh': 14, 'cinta': 15, 'sama': 16, 'meltryllis': 17}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(tokenizer.texts_to_sequences(['Saya Suka Programming']))\n",
        "print(tokenizer.texts_to_sequences(['Saya suka belajar programing sejak smp']))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JZW70cF9RWnk",
        "outputId": "e40031da-e699-49b1-ecd1-cc92a763c38c"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[3, 4, 2]]\n",
            "[[3, 4, 1, 1, 1, 1]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# padding on text\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "sequences_samapanjang = pad_sequences(sequences,\n",
        "                                      padding='post', #add padding in the end of text\n",
        "                                      maxlen=5, # max sequence length\n",
        "                                      truncating='post') #take the first maxlen words and discard the rest if words > maxlen\n",
        "print(sequences_samapanjang)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KCF7cVbOSHoF",
        "outputId": "add0acc6-0d5c-499d-a957-b07f4da30a19"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 3  4  2  0  0]\n",
            " [ 2  5  6  0  0]\n",
            " [ 7  8  9 10 11]\n",
            " [13 14  1  1  1]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Embedding"
      ],
      "metadata": {
        "id": "j_WJVJ6lUVlD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# embedding\n",
        "model = tf.keras.models.Sequential([\n",
        "    tf.keras.layers.Embedding(jumlah_kata, dimensi_embedding, panjang_input),\n",
        "    tf.keras.layers.Flatten(),\n",
        "    tf.keras.layers.Dense(24, activation='relu'),\n",
        "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "])"
      ],
      "metadata": {
        "id": "FFnYlSmyTAqq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "oxTNhW50UsHA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(padded_latih, label_latih,\n",
        "          epochs=num_epochs,\n",
        "          validation_data=(padded_test, label_test))"
      ],
      "metadata": {
        "id": "ue5oaMcmUsij"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}